#Aquatic Vegetation Project Work Team
#Master data set
#Submersed aquatic vegetation
#CSTARS ground truthing

#Nick Rasmussen
#nicholas.rasmussen@water.ca.gov

#NOTES: 
#2017: GPS coordinates are missing

#to do list
#move the dbf files to GitHub and pull them from there; easier to work with Shruti on it that way
#format the visual survey data
#for secchi depth, convert -99 to NA
#for secchi depth, also decide how to deal with measurements where 
#disc did not disappear before reaching bottom (ie, inaccurate readings)
#incorporate SAV spp from comments
  #includes trace spp and rare spp that should replace "SAV-Unknown"
#convert time to military format of pacific standard time
#consider changing NA for species to None
#species called "leafy sago" in notes is probably P. nodosus according to UCD
#figure out what it means when a spp is listed but with 0% rake cover (a common result)
  #are these trace amount spp?
#drop rows generated by structural zeros 
  #ie: rake_teeth_corr > 0 & species = NA & rake_prop=0
#consider adding column that indicates whether any SAV was collected
  #eg, 0 = open water, 1 = sav present

# Packages--------
library(tidyverse) #suite of data science tools
library(sf) #work with GPS coordinates
library(janitor) #make column names tidier
library(foreign) #read dbf files
library(hms) #formatting time
library(lubridate) #formatting dates

# Read in the data----------------------------------------------
# Data set is on SharePoint site for the 
# Delta Smelt Resiliency Strategy Aquatic Weed Control Action

#just start with 2021
#data on github
#this isn't working yet
#veg17 <- read_csv("https://github.com/InteragencyEcologicalProgram/AquaticVegetationPWT/blob/main/MasterDataSet_SAV/Data_Raw/CSTARS_GroundTruthing/Delta202107_fieldpoints.csv?raw=true")


# Define path on SharePoint site for data
# I synced this folder to my OneDrive
sharepoint_path_read <- normalizePath(
  file.path(
    Sys.getenv("USERPROFILE"),
    "California Department of Water Resources/DWR - DSRS Aquatic Weed Control Action - MasterDataSet_SAV/CSTARS_GroundTruthing"
  )
)  

#not working
#veg17 <- read_csv("Data_Raw/CSTARS_GroundTruthing/Suisun202107_fieldpoints.csv")

#Create character vector of all point data files
#there is also line data but we can work with that later
#pt_data_list <- dir(path = sharepoint_path_read, pattern = "\\_fieldpoints.dbf", full.names = T, recursive=T)

#Combine all of the data files into a single df
#column types generally match across data sets
#except that Rake_Teeth in 2016 is integer instead of factor
#this is because files for all other years include "%" in this column while 2016 does not
#start by combining 2017-2021 data
#sav_pt_data_most2 <- map_dfr(pt_data_list[2:6], ~read.dbf(.x), .id = "file_name")%>% 
  #remove the "%" from the Rake_Teeth column of the 2017-2021 df
  #so we can incorporate the 2016 df
#  mutate(Rake_Teeth= as.integer(str_replace(Rake_Teeth,pattern = "%", replacement = ""))) %>% 
#  glimpse()
#sav_pt_data_most <- pt_data_list[2:6] %>%
  #set_names() grabs the file names
## set_names() %>%  
  #reads in the files, .id adds the file name column
  #map_dfr(~read.dbf(.x), .id = "source") %>% 
  #reduce file name to just the needed info (ie, year)
  #mutate(year = as.integer(str_sub(source,-22,-19))
        # ,month = as.integer(str_sub(source,-18,-17))
         #remove the "%" from the Rake_Teeth column of the 2017-2021 df
         #so we can incorporate the 2016 df
         #,Rake_Teeth= as.integer(str_replace(Rake_Teeth,pattern = "%", replacement = ""))
         ) %>% 
 # glimpse()

#see how consistent column names are across years
#ie, are there extra columns created simply because of naming inconsistencies
#names(sav_pt_data_most) #generally looks consistent 

#read in 2016 data separately
#sav_pt_data16 <- read.dbf(file = paste0(sharepoint_path_read,"./Delta201610_fieldPointLines/Delta201610_fieldpoints.dbf"))
#sav_pt_data16 <- read.dbf(pt_data_list[1]) %>% 
  #add year and month columns 
 # add_column(year = as.integer("2016")
    #         ,month = as.integer("10")
    #         )

#combine 2016 df with 2017-2021 df
#sav_all <- bind_rows(sav_pt_data16,sav_pt_data_most)
#names(sav_all) 

#Rake spp data formatting-------------

#look at range of dates
unique(sav_all$GPS_Date) 
#figure out why there are NAs
#also correct "1899-12-30"

#look at rows with date in 1899
too_old <- sav_all %>% 
  filter(GPS_Date < "2016-01-01") %>% 
  group_by(year) %>% 
  summarize(count=n())
#75 obs from "1899-12-30"
#2018 = 59 and 2019 = 16

#look at rows with date = NA
d_miss <- sav_all %>% 
  filter(is.na(GPS_Date)) %>% 
  group_by(year) %>% 
  summarize(count=n()) %>% 
  summarize(sum(count))
#2017 = 66, 2018 = 59, 2019 = 71 (total = 196)
#for now, won't worry about the messed up dates
#I can get the month and year from the file names which is good enough short term
#after filtering to just the SAV data, see how many messed up dates there are

dpts <- sav_all %>% 
  #use janitor function to clean up column names
  clean_names() %>% 
  #rename some columns
  rename(date = gps_date
         ,time = gps_time
         ,feat = feat_name 
         #confirmed with UCD that units are feet
         ,depth_to_sav = depth_to_s
         #confirmed with UCD that units are feet
         ,secchi_depth = secchi_dep 
         #this column indicates whether disc hit bottom before disappearing
         ,secchi_bottom = secchi_b
  ) %>% 
  #convert feet to meters for depth measurements
  mutate(secchi_depth_m = secchi_depth*0.3048
         ,depth_to_sav_m = depth_to_sav*0.3048
         ) %>% 
  #removes all % from data frame but converts all columns to character which is a hassle
  #mutate_all(str_replace_all, "%", "")
  #instead remove % from specific columns and make those columns numeric
  mutate(rake_spec2 = str_replace(rake_spec2,pattern = "%", replacement = "") 
         ,rake_spec4 = str_replace(rake_spec4,pattern = "%", replacement = "")
         ,rake_spec6 = str_replace(rake_spec6,pattern = "%", replacement = "")
         ,rake_spec8 = str_replace(rake_spec8,pattern = "%", replacement = "")
         ,rake_spe10 = str_replace(rake_spe10,pattern = "%", replacement = "")
         ,across(c(rake_teeth,rake_spec2,rake_spec4,rake_spec6, rake_spec8, rake_spe10),as.numeric)
  ) %>% 
  #create new column that calculates total rake coverage of spp
  #should all be either 0 or 100; most but not all of them are, indicating some errors
  rowwise() %>% 
  mutate(tot_cover_spp = sum(c(rake_spec2,rake_spec4,rake_spec6, rake_spec8, rake_spe10),na.rm=T)) %>% 
  #make new columns that will allow for comparison of rake coverage to spp level coverage
  #values for rake_teeth_logical and tot_cover_spp_logical should match
  #ie, both be zero or both be one; mismatch indicates errors
  mutate(rake_teeth_logical = if_else(rake_teeth > 0, 1,0)
         ,tot_cover_spp_logical = if_else(tot_cover_spp > 0, 1,0)
         ,rake_diff = rake_teeth_logical-tot_cover_spp_logical
         #replace incorrect cases of rake_teeth=100% to rake_teeth=0%
         #this probably happened because 100% was default
         #there remain four cases in which there is a spp listed but with no cover %
         #likely the rake_teeth column was incorrectly used instead of the spp cover column
         #probably should just delete these four because we won't know the rake_teeth value
         #could later ask UCD to reference their field photos to correct these
         ,rake_teeth_corr = if_else((rake_diff==1 & is.na(rake_speci)),0,rake_teeth)
         ) %>% 
  #drop three rows where ssp cover % was incorrectly put into rake_teeth column
  filter(!(rake_diff==1 & !is.na(rake_speci))) %>% 
  #reduce to just the needed columns
  select(northing
         ,easting
         ,year
         ,month
         ,date
         ,time
         ,feat  
         ,depth_to_sav_m
         ,rake_teeth
         ,rake_teeth_corr
         ,rake_speci:rake_spe10
         ,secchi_depth_m
         ,secchi_bottom
         ,treated
         ,tot_cover_spp:rake_diff
         ,comments
  ) %>% 
  glimpse()

#look for cases in which values for rake_teeth_logical and tot_cover_spp_logical don't match
#note that I made corrections to some of the rake_teeth above but those would still show up
#in the rake_diff search 
dpts_check <- dpts %>% 
  filter(rake_diff!=0)
#no cases of rake_teeth being zero and spp cover being non-zero
#all are cases in which rake_teeth is non-zero while there is no spp specific cover data
#nearly all are rake_teeth = 100% with no spp specific cover
#presumably these should just be changed to rake_teeth=0%; nothing helpful in comments
#in three cases 0<rake_teeth<100 and there is a spp named in rake_spec1 column
#likely indicating that data were just entered into the wrong column

#histogram showing sum of all spp coverage on rake
#should be either 0 or 100
hist(dpts$tot_cover_spp)
cover_count<-dpts %>%
  group_by(tot_cover_spp) %>%
  summarize(count = n())
#most are either zero or 100 but there are various others, which indicates errors

#count the tot_cover_spp that are wrong
#ie, not zero or 100
cover_count_wrong<-dpts %>%
  filter(tot_cover_spp!=100 & tot_cover_spp!=0) %>% 
  group_by(tot_cover_spp) %>%
  summarize(count = n()) %>% 
  summarize(sum(count))
#72 samples with errors

#convert data frame from wide to long
dpts_long <- dpts %>% 
  #in prep for converting wide-ish to longest, rename some columns
  rename(rake_spec1 = rake_speci
         ,rake_prop1 = rake_spec2
         ,rake_prop3 = rake_spec4
         ,rake_prop5 = rake_spec6
         ,rake_prop7 = rake_spec8
         ,rake_prop9 = rake_spe10
  )  %>% 
  #convert the rake prop columns back to factors for converting from wide to long
  mutate(across(c(rake_prop1,rake_prop3,rake_prop5,rake_prop7,rake_prop9),as.factor)) %>% 
  #convert wide to long
  pivot_longer(cols="rake_spec1":"rake_prop9" #select range of columns
               , names_to = c("name","num") #specify column names
               , names_pattern = '([^0-9]+)([0-9]+)' #indicate where to split names (before and after numbers)
               , values_to = "value")  %>% 
  glimpse()

test <- dpts_long %>% 
  #now pivot back to a bit wider
  pivot_wider(names_from=name, values_from=value) %>% 
  glimpse()
  
  
  #drop unneeded columns: ssp num, original rake_teeth with errors,
  select(-c(num,rake_teeth,rake_teeth_logical:comments)) %>% 
  glimpse()

#looked at types of features
unique(dpts_long$feat)
#<NA>     SAV      EMR      Float    Riparian Unknown  SAV2     Point_ge

#look at number of samples per feature type
feat_count<-dpts_long %>% 
  distinct(year,date,time,feat) %>% 
  group_by(year,feat) %>% 
  summarize(count = n())
#feature is new column for 2021; NA for all other years
#in 2021, SAV is most abundant category, followed by EMR, float, Riparian
#other categories are rare: Point_ge SAV2 Unknown

#before dropping them, look at non-SAV feat types
fother <- dpts_long %>% 
  filter(feat!="SAV") 
unique(fother$rake_spec) #all NA which makes sense
  
#look at unique species
r_species <- dpts_long %>% 
  distinct(rake_spec)
unique(dpts_long$rake_spec)
#looks like we still have some rake coverage numbers mixed in with the species
#otherwise it looks good; a list of SAV species and some algae as expected

#look at cases in which species are % instead of names
#presumably these are just data entry error
#for now, will assume these should be NA for species
#but should check with UCD
ssp_chk <- dpts_long %>% 
  filter(grepl('%',rake_spec)) #148 rows

#look at number of samples for each % that should have been a spp
perc_count<-ssp_chk %>% 
  group_by(rake_spec) %>% 
  summarize(count = n())
#142 of 148 of these are 0%

# Making data frame with existing strings and their replacement
tr <- data.frame(target = c("SAV-S-naiad"        
                            ,"SAV-Egeria"
                            ,"SAV-Unknown"
                            ,"SAV-Elodea"
                            ,"SAV-Coontail"
                            ,"SAV-Watermilfoil"
                            ,"SAV-Rich-pondweed"
                            ,"SAV-Algae-mats"
                            ,"SAV-Sago-pondweed"  
                            ,"SAV-CrlLf-pondweed"
                            ,"SAV-Algae"
                            ,"SAV-Am-pondweed"    
                            ,"SAV-Cabomba"
                            ,"SAV-FnLf-pondweed"  
                            ,"SAV-Tapegrass"),
                 replacement = c("Najas_guadalupensis"        
                                 ,"Egeria_densa"
                                 ,"Unidentified"
                                 ,"Elodea_canadensis"
                                 ,"Ceratophyllum_demersum"
                                 ,"Myriophyllum_spicatum"
                                 ,"Potamogeton_richardsonii"
                                 ,"Algae"
                                 #confirmed with UCD this should be S. pectinata
                                 ,"Stuckenia_pectinata"  
                                 ,"Potamogeton_crispus"
                                 ,"Algae"
                                 ,"Potamogeton_nodosus"    
                                 ,"Cabomba_caroliniana" 
                                 ,"Stuckenia_filiformis"  
                                 ,"Vallisneria_australis"
                                 ))

# Making the named replacement vector from tr
replacements <- c(tr$replacement)
names(replacements) <- c(tr$target)

#continue formatting data set
dpts_cleaner <- dpts_long %>% 
  #drop rows generated by structural zeros for samples with SAV
  #ie: rake_teeth_corr > 0 & species = NA & rake_prop=0  
  filter(!(rake_teeth_corr>0 & is.na(rake_spec) & rake_prop=="0")) %>% 
  #also drop duplicates for cases when there is no sav
  #ie, collapse 5 rows to 1 row for the open water observations
  #NOTE: keep working on this; this didn't quite accomplish the goal
  #because there are cases within samples where rake_prop = 0 | NA
  filter(!duplicated(.)) %>% 
  #drop all feat types except SAV
  #already checked the others and they don't have SAV rake data (unsurprisingly)
  filter(feat=="SAV") %>% 
  #remove rows where a % is present instead of a species name
  #nearly all of these are "0%"
  filter(!grepl('%',rake_spec)) %>%  #148 rows
  #format spp names
  #clean up species names
  mutate(species = str_replace_all(rake_spec,pattern = replacements)) %>% 
  #add some columns
  add_column("program"="CSTARS"
             ,"survey_method"="rake_thatch") %>% 
  #change CRS of sample coordinates
  #specify the CRS of original coordinate
  #confirmed that it is UTM NAD 83 (Zone 10N) (EPSG = 26910)
  st_as_sf(coords = c("easting", "northing"), crs = 26910) %>%
  #then transform to WGS84
  st_transform(4236) %>% 
  #then convert from geometry to columns
  mutate(latitude_wgs84 = unlist(map(geometry,2)),
         longitude_wgs84 = unlist(map(geometry,1))) %>% 
  #drop the geometry column
  st_set_geometry(NULL) %>% 
  mutate(across(c(time),as.character)) %>% 
  #reorder columns, only keeping the necessary ones
  select("program"
         ,"latitude_wgs84"  
         ,"longitude_wgs84"
         ,"date"
         ,"time"
         ,"depth_to_sav_m"
         ,"survey_method"
         ,"rake_teeth_corr"
         ,"species"
         ,"rake_prop"
         ,"secchi_depth_m"    
         ,"secchi_bottom"   
         ) 

#look at depth to sav
hist(dpts_cleaner$depth_to_sav_m)
d_sav_count<-dpts_cleaner %>% 
  group_by(depth_to_sav_m) %>% 
  summarize(count = n())

#look at secchi depth
hist(dpts_cleaner$secchi_depth_m)
secchi_count<-dpts_cleaner %>% 
  filter(secchi_depth_m>=0) %>% 
  group_by(secchi_depth_m) %>% 
  summarize(count = n())
#a lot of -99 values
#confirmed with UCD these are just missing values
sec<-dpts_cleaner %>% 
  filter(secchi_depth_m>=0) 
hist(sec$secchi_depth_m)

#look at secchi to bottom
unique(dpts_cleaner$secchi_bottom)
bottom_count<-dpts_cleaner %>% 
  group_by(secchi_bottom) %>% 
  summarize(count = n())
#vast majority are "no" which makes sense because they sample in shallow areas
#that means "No" measurements aren't accurate secchi depth measurements

#look at treated
#according to UCD, this is just based on visual observation of foliage
#probably most relevant for FAV
#probably should just drop this column for SAV
#unique(dpts_cleaner$treated)
#treat_count<-dpts_cleaner %>% 
 # group_by(treated) %>% 
  #summarize(count = n())
#all are "no"

#write final data frame to csv file

# Define path on SharePoint site for output files
# I synced this folder to my OneDrive
sharepoint_path_write <- normalizePath(
  file.path(
    Sys.getenv("USERPROFILE"),
    "California Department of Water Resources/DWR - DSRS Aquatic Weed Control Action - MasterDataSet_SAV/Clean&Formatted"
  )
) 
#write_csv(dpts_cleaner,file = paste0(sharepoint_path_write,"/CSTARS_2021_formatted.csv"))

  
  
  